{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-21T15:35:09.206133Z",
     "start_time": "2026-02-21T15:35:09.195660Z"
    }
   },
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from docx import Document\n",
    "from urllib.parse import urlparse\n",
    "import json\n"
   ],
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T15:35:09.220853Z",
     "start_time": "2026-02-21T15:35:09.209671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "URLS_FILE = \"../urls/urls_mcq.txt\"     \n",
    "START_PAGE = 1\n",
    "MAX_PAGE = 50              \n",
    "DATA_DIR = Path(\"data_crawler\")\n",
    "DOC_DIR = Path(\"docs\")\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "DOC_DIR.mkdir(exist_ok=True)"
   ],
   "id": "9b0f9c98cadc0aca",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T15:35:09.230466Z",
     "start_time": "2026-02-21T15:35:09.222892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_topic_name(url):\n",
    "    path = urlparse(url).path.strip(\"/\")\n",
    "    return path.split(\"/\")[-1]\n",
    "\n",
    "def build_page_url(base_url, page):\n",
    "    if page == 1:\n",
    "        return base_url\n",
    "    return f\"{base_url.rstrip('/')}/page/{page}/\""
   ],
   "id": "cc33ec2b9a99e494",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T15:35:09.247017Z",
     "start_time": "2026-02-21T15:35:09.242525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # options.add_argument(\"--headless\")  # bật nếu muốn chạy ngầm\n",
    "\n",
    "    driver = webdriver.Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=options\n",
    "    )\n",
    "    return driver"
   ],
   "id": "4f973b1ee8d1ddcf",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T15:35:09.257881Z",
     "start_time": "2026-02-21T15:35:09.249027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "def crawl_one_page(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3) \n",
    "    try:\n",
    "        view_answer_buttons = driver.find_elements(By.XPATH, \"//button[contains(text(), 'View Answer')]\")\n",
    "\n",
    "        for btn in view_answer_buttons:\n",
    "            driver.execute_script(\"arguments[0].click();\", btn)\n",
    "\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi click nút View Answer: {e}\")\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    mcqs = []\n",
    "\n",
    "    questions = soup.select(\"div.questionContent\")\n",
    "\n",
    "    for q in questions:\n",
    "        q_text = q.select_one(\".questionContentText p\")\n",
    "        question = q_text.get_text(strip=True) if q_text else \"\"\n",
    "\n",
    "        options = [\"\", \"\", \"\", \"\"]\n",
    "        correct_answer = \"\"\n",
    "        explanation = \"\"\n",
    "\n",
    "        for opt in q.select(\".optionItem\"):\n",
    "            label = opt.select_one(\".optionIndex\")\n",
    "            content = opt.select_one(\".optionContent\")\n",
    "\n",
    "            if not label or not content:\n",
    "                continue\n",
    "\n",
    "            idx = ord(label.text.strip()) - ord(\"A\")\n",
    "            text = content.get_text(strip=True)\n",
    "\n",
    "            if 0 <= idx < 4:\n",
    "                options[idx] = text\n",
    "\n",
    "            classes = \" \".join(opt.get(\"class\", [])).lower()\n",
    "\n",
    "            if \"correct\" in classes or \"success\" in classes:\n",
    "                correct_answer = text\n",
    "\n",
    "        exp = q.select_one(\".explanationContent p\")\n",
    "        if exp:\n",
    "            explanation = exp.get_text(strip=True)\n",
    "\n",
    "        mcqs.append([\n",
    "            question,\n",
    "            options[0],\n",
    "            options[1],\n",
    "            options[2],\n",
    "            options[3],\n",
    "            correct_answer,\n",
    "            explanation\n",
    "        ])\n",
    "\n",
    "    return mcqs"
   ],
   "id": "1164fd4ea74e8d1",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T15:35:09.267518Z",
     "start_time": "2026-02-21T15:35:09.259894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_csv(mcqs, csv_path):\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            \"Question\",\n",
    "            \"Option A\",\n",
    "            \"Option B\",\n",
    "            \"Option C\",\n",
    "            \"Option D\",\n",
    "            \"Correct Answer\",\n",
    "            \"Explanation\"\n",
    "        ])\n",
    "        writer.writerows(mcqs)\n",
    "\n",
    "def csv_to_docx(csv_file, docx_file):\n",
    "    doc = Document()\n",
    "    doc.add_heading(csv_file.stem.replace(\"_\", \" \").title(), level=1)\n",
    "\n",
    "    with open(csv_file, encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        for index, row in enumerate(reader, start=1):\n",
    "            doc.add_paragraph(f\"{index}. {row['Question']}\")\n",
    "            options = [\n",
    "                row[\"Option A\"],\n",
    "                row[\"Option B\"],\n",
    "                row[\"Option C\"],\n",
    "                row[\"Option D\"],\n",
    "            ]\n",
    "\n",
    "            correct = row[\"Correct Answer\"]\n",
    "\n",
    "            for idx, text in enumerate(options):\n",
    "                if not text:\n",
    "                    continue\n",
    "                p = doc.add_paragraph()\n",
    "                run = p.add_run(f\"{chr(65+idx)}. {text}\")\n",
    "                if text == correct:\n",
    "                    run.bold = True\n",
    "\n",
    "            doc.add_paragraph(\"\")\n",
    "\n",
    "    doc.save(docx_file)"
   ],
   "id": "1b65571bc0f03e2c",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T15:35:31.952635Z",
     "start_time": "2026-02-21T15:35:09.267518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    with open(URLS_FILE, encoding=\"utf-8\") as f:\n",
    "        base_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    driver = init_driver()\n",
    "\n",
    "    for base_url in base_urls:\n",
    "        topic = get_topic_name(base_url)\n",
    "        print(f\"\\n Topic: {topic}\")\n",
    "\n",
    "        topic_data_dir = DATA_DIR / topic\n",
    "        topic_doc_dir = DOC_DIR / topic\n",
    "\n",
    "        topic_data_dir.mkdir(exist_ok=True)\n",
    "        topic_doc_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for page in range(START_PAGE, MAX_PAGE + 1):\n",
    "            page_url = build_page_url(base_url, page)\n",
    "\n",
    "            csv_path = topic_data_dir / f\"page_{page}.csv\"\n",
    "            docx_path = topic_doc_dir / f\"page_{page}.docx\"\n",
    "\n",
    "            if docx_path.exists():\n",
    "                print(f\" Skip {topic} page {page} (already crawled)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Crawling {topic} page {page}\")\n",
    "\n",
    "            try:\n",
    "                mcqs = crawl_one_page(driver, page_url)\n",
    "\n",
    "                if not mcqs:\n",
    "                    print(f\"No data, stop at page {page}\")\n",
    "                    break\n",
    "\n",
    "                save_csv(mcqs, csv_path)\n",
    "                csv_to_docx(csv_path, docx_path)\n",
    "\n",
    "                print(f\"{topic} page {page}: {len(mcqs)} questions\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error {topic} page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"\\n ALL DONE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "12f0384152555c3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Topic: docker\n",
      " Skip docker page 1 (already crawled)\n",
      " Skip docker page 2 (already crawled)\n",
      " Skip docker page 3 (already crawled)\n",
      " Skip docker page 4 (already crawled)\n",
      " Skip docker page 5 (already crawled)\n",
      "Crawling docker page 6\n",
      "No data, stop at page 6\n",
      "\n",
      " Topic: cloud-computing\n",
      " Skip cloud-computing page 1 (already crawled)\n",
      " Skip cloud-computing page 2 (already crawled)\n",
      " Skip cloud-computing page 3 (already crawled)\n",
      " Skip cloud-computing page 4 (already crawled)\n",
      " Skip cloud-computing page 5 (already crawled)\n",
      "Crawling cloud-computing page 6\n",
      "No data, stop at page 6\n",
      "\n",
      " Topic: linux\n",
      " Skip linux page 1 (already crawled)\n",
      " Skip linux page 2 (already crawled)\n",
      " Skip linux page 3 (already crawled)\n",
      " Skip linux page 4 (already crawled)\n",
      " Skip linux page 5 (already crawled)\n",
      "Crawling linux page 6\n",
      "No data, stop at page 6\n",
      "\n",
      " ALL DONE\n"
     ]
    }
   ],
   "execution_count": 133
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
